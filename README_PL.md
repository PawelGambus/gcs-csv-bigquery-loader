# â˜ï¸ GCP Cloud Function â€“ Åadowanie danych CSV do BigQuery

To repozytorium zawiera moje rozwiÄ…zanie zadania rekrutacyjnego polegajÄ…cego na zbudowaniu zautomatyzowanego pipelineâ€™u do Å‚adowania danych w Google Cloud Platform.

---

## ğŸ“ Opis zadania

Zaloguj siÄ™ do konsoli Google Cloud Platform i wybierz projekt: `alterdata-rekrutacja-46`.  
W tym projekcie masz peÅ‚ne uprawnienia.

### Zadanie

- UtwÃ³rz wÅ‚asny bucket w Google Cloud Storage o dowolnej nazwie.
- Zaimplementuj **Cloud Function (v1)** w GCP, ktÃ³ra:
  - bÄ™dzie wyzwalana po wrzuceniu pliku `.csv` do bucketu,
  - automatycznie stworzy odpowiadajÄ…cÄ… mu tabelÄ™ w BigQuery na podstawie wykrytego schematu (`autodetect=True`).

---

## ğŸš€ SzczegÃ³Å‚y mojego rozwiÄ…zania

### âœ… WybÃ³r regionu i konfiguracja zasobÃ³w

- WybraÅ‚em region `europe-central2 (Warszawa)` ze wzglÄ™du na niskie koszty oraz bliskoÅ›Ä‡ geograficznÄ… do firmy.
- ZarÃ³wno bucket, jak i Cloud Function znajdujÄ… siÄ™ w tym samym regionie, aby uniknÄ…Ä‡ kosztÃ³w przesyÅ‚u miÄ™dzy regionami.
- WyÅ‚Ä…czyÅ‚em replikacjÄ™ wieloregionowÄ…, aby ograniczyÄ‡ zbÄ™dne koszty.

### âœ… Cloud Function â€“ Trigger i architektura

- UÅ¼yÅ‚em triggera: `google.cloud.storage.object.v1.finalized`  
  â†’ Wyzwalany po zakoÅ„czeniu uploadu pliku (na podstawie [dokumentacji GCP](https://cloud.google.com/functions/docs/calling/storage)).
- ZweryfikowaÅ‚em, Å¼e upload pliku faktycznie uruchamia funkcjÄ™.
- Funkcja zostaÅ‚a skonfigurowana z minimalnymi zasobami i ograniczeniem do jednego rÃ³wnoczesnego wykonania, zakÅ‚adajÄ…c niskie obciÄ…Å¼enie.

### âœ… Dataset i obsÅ‚uga tabel BigQuery

- UtworzyÅ‚em dataset za pomocÄ… CLI.
- Zamiast biblioteki pandas uÅ¼yÅ‚em natywnej biblioteki Google Cloud SDK dla Pythona, aby uproÅ›ciÄ‡ integracjÄ™ i uniknÄ…Ä‡ dodatkowych zaleÅ¼noÅ›ci.
- SprawdziÅ‚em istnienie tabel na podstawie [tego wzorca z dokumentacji BigQuery](https://cloud.google.com/bigquery/docs/samples/bigquery-table-exists).
- PrzetworzyÅ‚em i zweryfikowaÅ‚em nazwy tabel zgodnie z [restrykcjami BigQuery](https://cloud.google.com/bigquery/docs/tables).
- W przypadku konfliktu nazw dodaÅ‚em automatyczne numerowanie tabel (np. `tabela_2`, `tabela_3`).
- PomijaÅ‚em pliki, ktÃ³re nie sÄ… typu CSV lub znajdujÄ… siÄ™ w nieobsÅ‚ugiwanych folderach.

### âœ… Konfiguracja Å‚adowania danych

- UÅ¼yÅ‚em nastÄ™pujÄ…cej konfiguracji `LoadJobConfig`:
  ```python
  job_config = bigquery.LoadJobConfig(
      autodetect=True,
      column_name_character_map="V2",
      skip_leading_rows=1,
      source_format=bigquery.SourceFormat.CSV,
  )
  ```
  - `column_name_character_map="V2"`: automatycznie poprawia problematyczne nagÅ‚Ã³wki kolumn, aby byÅ‚y zgodne z BigQuery.
  - Na podstawie [oficjalnej dokumentacji](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.job.LoadJobConfig#google_cloud_bigquery_job_LoadJobConfig_column_name_character_map).

### ğŸ§¹ Walidacje i usprawnienia

- ZweryfikowaÅ‚em, Å¼e wrzucany plik faktycznie ma rozszerzenie `.csv`.
- OczyÅ›ciÅ‚em nazwy tabel i sprawdziÅ‚em je pod kÄ…tem niedozwolonych znakÃ³w lub nadmiernej dÅ‚ugoÅ›ci.
- UpewniÅ‚em siÄ™, Å¼e nagÅ‚Ã³wki sÄ… poprawne, a format pliku CSV jest spÃ³jny.
- W przypadku istnienia tabeli o tej samej nazwie dodawany jest sufiks.
- Logowanie odbywa siÄ™ wewnÄ…trz funkcji Cloud Function za pomocÄ… `print()` â€“ choÄ‡ docelowo powinno byÄ‡ przekierowane do logÃ³w strukturalnych.

---

## ğŸ”’ Uprawnienia i bezpieczeÅ„stwo

- Po zakoÅ„czeniu testÃ³w wyÅ‚Ä…czyÅ‚em **publiczny dostÄ™p** do bucketu.
- BrakujÄ…cÄ… rolÄ™ `roles/pubsub.publisher` dodaÅ‚em podczas debugowania.

---

## ğŸ§ª Wykorzystane technologie

- Google Cloud Storage
- Google Cloud Functions (Python v1)
- BigQuery + CLI (`bq`)
- Python BigQuery SDK

---

## ğŸ§‘â€ğŸ’» Autor

Pawel Gambus  
[LinkedIn](https://www.linkedin.com/in/pawel-gambus)
